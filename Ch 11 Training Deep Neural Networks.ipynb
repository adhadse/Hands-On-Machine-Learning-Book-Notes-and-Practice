{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch 11 Training Deep Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhadse/colab_repo/blob/master/homl/Ch%2011%20Training%20Deep%20Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45H773qOr8C3"
      },
      "source": [
        "#Chapter 11: Training Deep Neural Networks\n",
        "This work is partialy combined text and code from the book [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) is only supposed to be used as reference and is recommended to follow along with a copy of the Book puchased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj9iALEBSKQj"
      },
      "source": [
        "# The Vanishing/Exploding Gradients Problems\n",
        "Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower laers connection weights almost unchanged. Which leads to the training not converging to optimal solution. This is called <mark>*vanishing gradients*</mark>.\n",
        "\n",
        "Other times, the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is referred to as <mark>*exploding gradients*</mark>.\n",
        "\n",
        "**DNNs suffer from unstable gradients.**\n",
        "\n",
        "In a 2010 paper, by Xavior Glorot and Yoshua Begio; the authors that due to few reasons prominent in those times like the use of  logistic activation function and the weight initialization technique common during that time (a normal distribution with a mean of 0 and a standard deviation of 1), they concluded <mark>the varaince if the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers</mark>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi8uC2fKBSz1"
      },
      "source": [
        "## Glorot initialization and He initialization\n",
        "The paper also proposed a way to significantely alleviate the unstable gradients problem. The signal needs to properly flow, to do so:\n",
        "- <mark>We need the variance of the outputs of each layer to be equal to the variance of its inputs.</mark>\n",
        "- <mark>We also need the gradients to have equal variance before and after flowing through a layer in the reverse direction<mark>\n",
        "\n",
        "This is not possible unless we have equal number of inputs and neurons ( these are called *fan-in* and *fan-out*. But Glorot and Bengio proposed a good compromise, **initialize connection weights of each layer randomly**, as per equation 11-1 where, $fan_{avg} = \\frac{fan_{in} + fan_{out}}{2}$. This initialization strategy is called <mark>*Xavier initialization*</mark> or <mark>*Glorot initialization*</mark>. \n",
        "\n",
        "**Equation 11-1 Glorot Initialization (When using logistic Activation function)**\n",
        "\n",
        "- Normal distribution with mean 0 and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
        "\n",
        "- Or a uniform distribution between $-r$ nad $+r$, with $r=\\sqrt{\\frac{3}{fan_{avg,}}}$\n",
        "\n",
        "If we replace $fan_{avg}$ with $fan_{in}$ in the Equation 11-1 we get initialization strategy propsed in the 1990s. He called it LeCun Initialization. It is equivalent to Glorot Initialization when $fan_{in}=fan_{out}$. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning.\n",
        "\n",
        "<mark>The initialization strategy for the ReLU activation function (and its variants, including the ELU activation) is sometimes calle *He initialization*</mark>\n",
        "\n",
        "**Table 11-1, Initialization parameters for each type of activation function**\n",
        "\n",
        "|Initialization| Activation function|$\\sigma^2$ (Normal)|\n",
        "|---|---|---|\n",
        "|Glorot| None, tanh, logistic, softmax| $\\frac{1}{fan_{avg}}$|\n",
        "|He|ReLU and variants|$\\frac{2}{fan_{in}}$ |\n",
        "|LeCun|SELU|$\\frac{1}{fan{in}}$|\n",
        "\n",
        "By default, Keras uses Glorot initialization with a uniform distribution. When creating a layer, we can change this to He Initialization by setting `kernel_initializer=\"he_uniform\"` or `kernel_initializer=\"he_normal\"` like this:\n",
        "\n",
        "`keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")`\n",
        "\n",
        "If we want He initialization with a uniform distribution but based o $fan_{avg}$ rather than $fan_{in}$, we can use the `VarianceScaling` initializer like this:\n",
        "\n",
        "```Python\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri4y0ioli4M0"
      },
      "source": [
        "## Nonsaturating Activation Functions\n",
        "Unfortunately, the ReLU activation function is not perfect. It suffers from a probelm known as the *dying ReLUs* during training. <mark>A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set.</mark>. Gradient Descent also doesn't affect because for the gradient of  ReLU activation function is zero when its output is negative.\n",
        "\n",
        "### Solution\n",
        "Use a variant of the ReLU activation function such as:\n",
        "1. **Leaky ReLU**:\n",
        "$$\\text{LeakyReLU}_{\\alpha}(z) = \\max(\\alpha z,\\, z)$$\n",
        "The hyperparamter $\\alpha$ defines how much the function \"leaks\": it is the slope of the function for $z< 0 $ and is typically set to 0.01. This small slope ensures that neurons dont' die, may go into comma, but have a chance to wake up. Outperformed ReLU.\n",
        "\n",
        "2. **Randomized Leaky ReLU (RReLU)**:\n",
        "$\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing. Seems to act as regulizer.\n",
        "\n",
        "3. **Parametric Leaky ReLU (PReLU)**: \n",
        "$\\alpha$ is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation). Strongly outperform ReLU on large image datasets, but on smaller risks overfitting. \n",
        "\n",
        "4. **Exponential Linear Unit (ELU)**:\n",
        "Outperformed all the ReLU variants in the authors' experiments.\n",
        "\n",
        "**Equation 11-2. ELU activation function**\n",
        "$$\\text{ELU}_{\\alpha}(z) = \\begin{cases}\n",
        "                            \\alpha(\\exp (z) -1) & \\text{if } z < 0 \\\\\n",
        "                            z  &\\text{if } z>=0\n",
        "\\end{cases}$$\n",
        "There are major differences with respect to ReLU activation function:\n",
        "- Has nonzero gradients for $z < 0$, which <mark>avoids the dead neurons problem.</mark>\n",
        "- If $\\alpha$ is equal to 1 then the function is smooth everywhere, including around  $z=0$, which helps spped up Gradient Descent since it <mark>does not bounce as much to the left or right of $z=0$.</mark>\n",
        "- It takes on negative values when $z< 0$ which allows the unit to have an average output closer to 0 <mark> and helps alleviate the vanishing gradients problem</mark>.\n",
        "\n",
        "The **main drawback** of the ELU activation function is that it is **slower to compute than the ReLU function** (and its variants) due to the use of the exponential function). Its faster convergence rate during training compensates for the slow computation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JJfKB8h4EQ8"
      },
      "source": [
        "Then, a 2017 paper by Gunter klambauer et al. introduced the <mark>Scaled ELU (SELU)</mark> activation function, which is a scaled variant of the ELU activation function. The authors showed that if you build a neural netowrk composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will *self-normalize*. \n",
        "\n",
        "There are, however, a few conditions for self-normalization to happen:\n",
        "- <mark>The input features must be standardized (mean 0 and standard deviation 1).</mark>\n",
        "- <mark>Everu hidden layer's weights must be initialized with LeCun normal initialization. (set `kernel_initializer=\"lecun_normal\"`.</mark>\n",
        "- <mark>The network's architecture must be sequential.</mark> Will not gurantee self-normalization with *skip connection*.\n",
        "\n",
        "> ðŸŸ¢ **Which one to choose?**\n",
        "> \n",
        "> **SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic**\n",
        ">\n",
        "> - If network architecture prevents from self-normalization, then ELU may perform better than SELU (since, SELU is not smooth at $z = 0$)\n",
        "> - Care about runtime latency? go for leaky ReLU.\n",
        "> - Try (if have spare time and compute rsc) RReLU if overfitting, PReLU if you have huge training set.\n",
        "\n",
        "**How to use Leky ReLU activation function in Keras?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ggUv92oyi53"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCt9XIOv1L6e"
      },
      "source": [
        "model = keras.model.Sequential([\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7RSEWLVAHtl"
      },
      "source": [
        "**How to implement PReLU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PcxW2rER0wB"
      },
      "source": [
        "model = keras.model.Sequential([\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.PReLU(alpha=0.2)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diq9YFqOAadk"
      },
      "source": [
        "There isn't currently official implementation of RReLU in Keras but we can easily implement.\n",
        "\n",
        "**For SELU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG8NKbZrAu8t"
      },
      "source": [
        "layers = keras.layers.Dense(10, activation=\"selu\",\n",
        "                            kernel_initializeer=\"lecun_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWBgnPTBA_ee"
      },
      "source": [
        "## Batch Normalization\n",
        "He initialization along with ELU doesn't gurantee that the vanishing/exploding gradients problems won't come back during training (only in the beginning).\n",
        "\n",
        "Then in a 2015 paper, Sergey Ioffe and Christian Szegedy propsed a technique all *Batch Normalization (BN)*. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. In simple words, the operation <mark>lets the model learn the optimal scale and mean of each of the layers's inputs.</mark>\n",
        "\n",
        "**In a nutshell**\n",
        "\n",
        "<mark>The operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting</mark>\n",
        "\n",
        "**Working**\n",
        "\n",
        "The algorithm needs to estimate each input's mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch. \n",
        "\n",
        "**Equation 11-3. Batch Normalization algorithm**\n",
        "1. $\\mu_B = \\frac{1}{m_B}\\;\\sum\\limits_{i=1}^{m_B}x^{(i)}$\n",
        "2. $\\sigma_{B}^2 = \\frac{1}{m_B}\\;\\sum\\limits_{i=1}^{m_B}\\big(x^{(i)} - \\mu_B\\big)^2$\n",
        "3. $\\hat{x}^{(i)} = \\frac{x^{(i)} \\,-\\, \\mu_B}{\\sqrt{\\sigma_B^2\\, + \\,\\epsilon}}$\n",
        "4. $z{{(i)} = \\gamma \\otimes\\hat{x}^{(i)}\\, +\\,\\beta}$\n",
        "\n",
        "In this algorithm, \n",
        "- $\\mu_B$ is the <mark>vector of input means</mark>, evaluated over the whole mini-batch $B$ (it contains one means per input of mini-batch)\n",
        "- $\\mu_B$ is the <mark>vector of input standard deviatitions</mark>, also evaulated over the whole mini-batch (it contains one standard deviation per input of mini-batch)\n",
        "- $m_B$ is the <mark>number of instances in the mini-batch</mark>.\n",
        "- $\\hat{x{(i)}}$ is the <mark>vector of zero-centered and normalized inputs for instance $i$</mark>.\n",
        "- $\\gamma$ is the <mark>output scale parameter vector for the layer (it contains one scale parameter per input of mini-batch).</mark>\n",
        "- $\\otimes$ represents <mark>element wise multiplication </mark>(each input is multiplied by its corrensponding output scale parameter).\n",
        "- $\\beta$ is the <mark>output shift (offset) parameter vector for the layer </mark>(it contains one offset parameter per input of mini-batch). Each input is offset by its corresponding shift parameter.\n",
        "- $\\epsilon$ is a <mark>tiny number that avoids division by zero</mark> (typically $10^{-5}$. This is called <mark>*smoothing term*</mark>\n",
        "- $z^{(i)}$ is the <mark>output of the BN operation</mark>. It is scaled and shifted version of the inputs.\n",
        "\n",
        "When we need to make predictions for individual instances, we will not have inputs' mean and standard deviation. Even if we batch the input test instances the instances themselves may not be independent and identically distributed.\n",
        "\n",
        "One solution could be to wait until the training finishes, then run the whole training set through the neural netowork and compute the mean and standard deviation of each input of the BN layer. However, most implementation (including keras) of BN estimate these final statistics during traingn by using a moving average of the layer's input means and standard deviation. \n",
        "\n",
        "Four parameter are learned in each BN layer: $\\gamma$ & $\\beta$ are learned through regular backpropagation, and $\\mu$ & $\\sigma$ are estimated using an exponential moving average. <mark>Note that $\\mu$ & $\\sigma$ are estiamted during training, BUT are used only after training (to replace the batch input means and standard deviations in Equation 11-3).</mark>\n",
        "\n",
        "### Implementing Batch Nomalization with Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3wszJ139aTE"
      },
      "source": [
        " model = keras.models.Sequential([\n",
        "      keras.layers.Flatten(input_shape=(28, 28)),\n",
        "      keras.layers.BatchNormalization(),\n",
        "      keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "      keras.layers.BatchNormalization(),\n",
        "      keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "      keras.layers.BatchNormalization(),\n",
        "      keras.layers.Dense(10, activation=\"softmax\")\n",
        " ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-ueHi7A52mr",
        "outputId": "9b54162b-03df-43f2-bfd0-9af3ceadeb65"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Cez-um54k2",
        "outputId": "6e119f04-5973-46c2-e29f-48c6bec4ab1c"
      },
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization/gamma:0', True),\n",
              " ('batch_normalization/beta:0', True),\n",
              " ('batch_normalization/moving_mean:0', False),\n",
              " ('batch_normalization/moving_variance:0', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcXKsQwr_Glu"
      },
      "source": [
        "Creating BN layers in Keras, it also creates two operations (TF operations) that will be called by Keras at each iteration during training.\n",
        "\n",
        "There is some debate about the preferred way to place BN layers, before or after. <mark>To add the BN layers before the activation functions, you must remove the activation function from the hidden layers and add them as separate layers after the BN layers.</mark>\n",
        "Since the BN layer already includes one offset parameter per input, we can remove the bias term from the previous layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbjl-I-59_wr"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layer.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhbeDu3TFE6w"
      },
      "source": [
        "`BatchNormalization` class has quite a few hyperparameters you can tweak. (Defaults are also goo) <mark>`momentum` is one such hyperparameter which is used by the `BatchNormalization` layer when it updates the exponential moving averages;</mark> given a new value <mark>$\\textbf{v}$</mark> (i.e., **a new vector of input means or standard deviations** computed over the current batch), the layer updates the **running average <mark>$\\hat{\\textbf{v}}$</mark>** using the following equation:\n",
        "\n",
        "$$\\hat{v}\\,\\leftarrow\\,\\hat{v}\\times\\text{momentum} + v \\times (1- \\text{momentum})$$\n",
        "\n",
        "A good momentum value is typically close to 1; for example, 0.9, 0.99, or 0.9999 (we would want more 9s for larger datasets and smaller mini-batches.\n",
        "\n",
        "\n",
        "Another important hyperparameter is `axis`: <mark>which determines which axis should be normalized.</mark> It defaults to -1, meaning that by default it will normalize the last axis (using the means and standard deviations computed across the *other* axis.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd3jRs7hq5mL"
      },
      "source": [
        "## Gradients Clipping\n",
        "Another popular technique to mitgate the exploding gradients problem is to <mark>clip the gradients during backpropagation so that they never exceed some threshold.</mark> This is called *gradients clipping*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZOy5mJtogpc"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faOXbDzUr0vT"
      },
      "source": [
        "This means that all the partial derivatives of the loss (with regard to each and every trainaible parameter) will be clipped between -1.0 and 1.0. Note that it may change the orientation of the gradeint vector.\n",
        "\n",
        "If you want to ensure that Gradient Clipping does not chenage the direction of the gradient vector, you should clip by norm by setting `clipnorm` instead. This will clip the whole gradient if its $\\mathcal{l_2}$ norm is greater than the threshold you picked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A5PP6VqIFBK"
      },
      "source": [
        "# Reusing Pretrained Layers\n",
        "Insterad of training very large DNNs from scratch: instead, we should always try to find an existing neural entowrk that accomplished a similar task to the one we are trying to tackle. This is what is referred to as transfer learning.\n",
        "\n",
        "> ðŸ”µ If the input pictures of your task don't have the same size as the ones used in the original task, you will usually ave to add a preprocessing step to resize them to the size expected by the original model.\n",
        "\n",
        "> ðŸŸ¢ The more similar the tasks are, the more layers you want to reuse (starting with the lower layers).\n",
        "\n",
        "First off keep all the tranferred layer freezed (so that gradient descent don't do its magic), see how the model performs. Then unfreeze few top layers, and then revaulate. <mark>Also, keep the learning rate low, so that the fine-tuned weights dot get wrecked</mark>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LwebUy-Ny2f"
      },
      "source": [
        "## Transfer Learning with Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pImqZdAG7kLI"
      },
      "source": [
        "# original model will share layers\n",
        "model_A = keras.model.load_model(\"my_model_A.h5\")\n",
        "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
        "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoUSTngoJ7C9"
      },
      "source": [
        "Note that `model_A` and `model_B_on_A` now **shares some layers**. When you train `model_B_on_A`, **it will also affect `model_A`**. To avoid that, you need to *clone* `model_A` before you reuse its layers. <mark>To do this, you clone model A's architecture with `clone.model()`, then copy its weights (since `clone_mode()` does not clone the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4_CdVw9PplL"
      },
      "source": [
        "model_A_clone = keras.models.clone_model(model_A)\n",
        "model_A_clone.set_weights(model_A.get_weights())\n",
        "\n",
        "# Now the model we want to train will share layers with copied model of modelA\n",
        "model_B_on_A = keras.model.Sequential(model_A_clone.layers[:-1])\n",
        "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49XzrCZlP5M9"
      },
      "source": [
        "But since the output layer it is initialized randomly, it will make large errors (at least during the first few epochs), so there will large error gradients that may wreck the reused layers weights. To avoid this we will *freeze* the reused layers during the first few epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3quys8X91Qm"
      },
      "source": [
        "for layer in model_B_on_A.layers[:-1]:\n",
        "  layer.trainable = False\n",
        "\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", \n",
        "                     optimizer=\"sgd\",\n",
        "                     metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5PcT_2GbJjY"
      },
      "source": [
        "> ðŸ”µ **You must always compile your model after you freeze or undreeze layers.**\n",
        "\n",
        "After unfreezing the reused layers, it is ususally a good idea to reduce the learning rate, once again to avoid daamaging the reused weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NVRvbjsbUxV"
      },
      "source": [
        "history = model_B_on_A.fit(X_train_B, y_train_B, \n",
        "                           epochs=4,\n",
        "                           validation_data=(X_valid_B, y_valid_B))\n",
        "\n",
        "\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "  layer.trainaible = True\n",
        "\n",
        "optimizer = keras.optimizers.SGD(lr=1e-4)\n",
        "model_B_on_A.compile(loss=binary_crossentropy, \n",
        "                     optimizer=optimizer,\n",
        "                     metrics=[\"accuracy\"])\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, \n",
        "                           epochs=16,\n",
        "                           validation_data=[X_valid_B, y_valid_B])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v2IghUV5uVY"
      },
      "source": [
        "## Unsupervised Pretraining\n",
        "Let's suppose we need to figure out a complex task and we don't have enough data and also we cannot find a model trained on a similar task. \n",
        "\n",
        "What we can is utilize *unsupervised pretraining*. \n",
        "- <mark>Gather unlabeled trainig example</mark>\n",
        "- <mark>Try to use it to train an unsupervised model</mark>\n",
        "  \n",
        "  Such as an autoencoder or Generative adversarial Network. \n",
        "- <mark>Reuse the lower layers of the GAN's discriminator</mark>\n",
        "\n",
        "  Add the output layer for your task, and fine tune the final network using the supervised learning (i.e., with the labeled training examples).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD_S96-9Lt63"
      },
      "source": [
        "# Faster Optimizers\n",
        "So far we have seen four ways to speed up training (and reach a better solution):\n",
        "- Applying a good initialization strategy for the connection weights\n",
        "- Using a good activation function\n",
        "- Using Batch Normalization\n",
        "- Reusing parts of a pretrained network\n",
        "\n",
        "Another huge speed boost speed comes from using a faster optimizer than the regular Gradient Descent Optimizer. We will look at most popular algorithms:\n",
        "- [Momentum Optimization](#momentum-optimization)\n",
        "- [Nesterov Accelerated Gradient](#nesterov-accelerated-gradient)\n",
        "- [AdaGrad](#adagrad)\n",
        "- [RMSProp](#rmsprop)\n",
        "- [Adam and Nadam optimization](#adam-and-nadam-optimization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw-P8dU9M-JY"
      },
      "source": [
        "## <a name=\"momentum-optimization\"></a>Momentum Optimization\n",
        "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity.\n",
        "\n",
        "Where Gradient Descent doesn't care about what previous gradients were:\n",
        "$$\\theta \\,\\leftarrow\\,\\theta - \\eta\\;\\nabla_{\\theta}J(\\theta)$$\n",
        "\n",
        "Where, it updates <mark>weights $\\theta$</mark> by directly subtracting the <mark>gradient of cost function $j(\\theta)$ with regard to the weights ($\\nabla_\\theta j(\\theta)$)</mark> multilied by the <mark>learning rate $\\eta$.</mark>\n",
        "\n",
        "### Momentum Optimization uses gradient is used for accelearation, not for speed.\n",
        "*Equation 11-4. Momentum Algorithm*\n",
        "$$\\textbf{m} \\leftarrow \\beta \\textbf{m} - \\eta\\;\\nabla_\\theta J(\\theta$$\n",
        "$$\\theta \\leftarrow \\theta + \\textbf{m}$$\n",
        "\n",
        "At each iteration, it substract the local gradient from the *momentum vector* $\\textbf{m}$ (mulitplied by the learning rate $\\eta$). Then it updates the weights by adding this momentum vector.\n",
        "\n",
        "To prevent the momentum from growing too large and to simulate some sort of friction mechanism the hyperparameter $\\beta$ is used called *momentum* which must be set between 0 (high friction) and 1 (no friction). Typically set to 0.9.\n",
        "\n",
        "**Implementing in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyfJF3dVOsCT"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYRWRVMRiKmm"
      },
      "source": [
        "## <a name=\"nesterov-accelerated-gradient\"></a> Nesterov Accelerated Gradient\n",
        "<mark>**Measures the gradient of the cost function not at the local position $\\theta$ but slightly ahead in the direction of the momentum, at $\\theta + \\beta\\textbf{m}$**</mark>\n",
        "\n",
        "*Equation 11-5. Nesterov Accelerated Gradient Algorithm* \n",
        "$$\\textbf{m}\\leftarrow \\beta\\textbf{m} - \\eta\\;\\nabla_\\theta J(\\theta+\\beta\\textbf{m})$$\n",
        "$$\\theta \\leftarrow \\theta + \\textbf{m}$$\n",
        "\n",
        "**Implementing in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSVsJzpXiVEC"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPYZNRYXqqyr"
      },
      "source": [
        "## <a name=\"adagrad\"></a>AdaGrad\n",
        "Gradient Descent always go head over heels down towards the direction which points to the steepest slope, and not in direction of global optimum. <mark>It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum.</mark> The *AdaGrad* algorithm achieves this correction by **scaling down the gradient vector along the steepest dimensions.**\n",
        "\n",
        "*Equation 11-6. AdaGrad algorithm*\n",
        "$$\\textbf{s} \\leftarrow \\textbf{s} + \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta)$$\n",
        "$$\\theta \\leftarrow \\theta - \\eta \\;\\nabla_\\theta J(\\theta)\\;\\oslash\\sqrt{\\textbf{s}+\\epsilon}$$\n",
        "\n",
        "> **NOTE**\n",
        ">\n",
        "> 1. $\\otimes$ represent element-wise multiplication\n",
        "> 2. $\\oslash$ resprent element-wise division.\n",
        "\n",
        "1. The first step accumulates the square of the gradients into the vector $\\textbf{s}$. \n",
        "2. The second step is almost identical to Gradient Descent, but with one key difference; the gradient vector is scaled down by a factor of $\\sqrt{\\textbf{s}+\\epsilon}$. $\\epsilon$ is a smooting term to avoid division by zero, typically around to $10^{-10}$. \n",
        "\n",
        "<mark>In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for the gentler slopes. This is called an *adaptive learning rate*.</mark>\n",
        "\n",
        "Adagrad has a disadvantage of slowing down a bit too fast and stopping too early; and so they are not very helpful to train deep neural networks. (although it may be efficient for simpler tasks such as Linear Regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJnTb5bo_gcF"
      },
      "source": [
        "## <a name=\"rmsprop\"></a> RMSProp\n",
        "The RMSProp fixes AdaGrad by **accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training)**. It does so **by using exponential decay in the first step**.\n",
        "\n",
        "*Equation 11-7. RMSProp algorithm*\n",
        "$$\\textbf{s} \\leftarrow \\beta s + (1-\\beta)\\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta)$$\n",
        "$$\\theta\\leftarrow\\theta - \\eta\\nabla_\\theta J(\\theta) \\oslash \\sqrt{\\textbf{s}+\\epsilon}$$\n",
        "\n",
        "The <mark>decay rate $\\beta$ is typically set to 0.9.</mark>  \n",
        "\n",
        "**Implementing in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxEIZ7qKGXdP"
      },
      "source": [
        "optimizer = keras.optimizers.RMSProp(lr=0.001, rho=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyzrYot7G8LC"
      },
      "source": [
        "## <a name=\"adam-and-nadam-optimization\"></a> Adam and Nadam Optimization\n",
        "*Adam* which stands for ***adaptive moment estimation***, is a adaptive learning rate algorithm that combines the ideas of momentum optimization and RMSProp: \n",
        "- Just like momentum optimization, it keeps track of an exponentially decaying average of past gradients\n",
        "- and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients.\n",
        "\n",
        "*Equation 11-8. Adam algorithm*\n",
        "$$\\textbf{m} \\leftarrow \\beta_1\\textbf{m} - (1 - \\beta_1)\\nabla_\\theta J(\\theta)$$\n",
        "$$\\textbf{s} \\leftarrow \\beta_2\\textbf{s} + (1-\\beta_2)\\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta)$$\n",
        "$$\\hat{\\textbf{m}} \\leftarrow \\frac{\\textbf{m}}{1 -\\mathbf{\\beta}^\\top_1}$$\n",
        "$$\\hat{\\textbf{s}} \\leftarrow \\frac{\\textbf{m}}{1 - \\mathbf{\\beta}^\\top_2}$$\n",
        "$$\\theta \\leftarrow \\theta + \\eta\\,\\hat{m}\\oslash \\sqrt{\\textbf{s} + \\epsilon}$$\n",
        "\n",
        "In this equation :\n",
        "- <mark>$\\top$ represents the iteration number.</mark>\n",
        "- <mark>$\\beta_1$ is momentum decay hyperparameter typically set to 0.9</mark>\n",
        "- <mark>$\\beta_2$ is scaling decay hyperparameter typically set to 0.999</mark>\n",
        "- <mark>A smoothing parameter $\\epsilon$ is usually initialized to a tiny number such as $10^{-7}$\n",
        "\n",
        "\n",
        "\n",
        "**Implementing in keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZV6c591HDA_"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3RC5mr4FwIB"
      },
      "source": [
        "Two variants of Adam are worth mentioning:\n",
        "\n",
        "- *AdaMax*\n",
        "  \n",
        "  Adam, replaces the $\\ell_2$ norm with the $\\ell_\\infty$ norm, specifically, it replaces step 2 in Equation 11-8 with \n",
        "  $$\\mathbf{s}\\leftarrow \\max(\\beta_s \\mathbf{s},\\;\\nabla_\\theta J(\\theta))$$, it drops step 4, and in step 5 it scales down the gradient updates by a factor od $\\mathbf{s}$, which is just the max of the time-decayed gradients.\n",
        "\n",
        "- *Nadam*\n",
        "\n",
        "  Nadam optimization is Adam Optimization plus the Nesterov trick, so it will often converge slightly faster than Adam. \n",
        "\n",
        "> ðŸŸ  Adaptive optimization methods are often freat, converge faster than normal gradient descent, <mark>but they can lead to solutions that generalize poorly on some datasets.</mark> So when you are disappointed by your model's performance, just try plain Nesterov Accelerated Gradient instead.\n",
        " \n",
        "> ## Training Sparase Models\n",
        "> All the optimization algorithms just presented produce <mark>dense models, meaning that most parameters will be nonzero.</mark> If you need blazingly fast model at runtime, or if you need to take up less memory, you may want to a sparse model instead, whcih has very few nonzero parameters.\n",
        ">\n",
        "><mark>A very good option is to apply strong $\\ell_1$ regularization during training</mark>, as it pushes the optimizer to zero out as many weights as it can. Otherwise check out TensorFlow Model Optimization Toolkit (TF-MOT), which provides a pruning API.\n",
        "\n",
        "\n",
        "\n",
        "*Table 11-2. Optimizer comparison* (ðŸŸ©â¬œâ¬œ is bad, ðŸŸ©ðŸŸ©â¬œ is average and ðŸŸ©ðŸŸ©ðŸŸ© is good)\n",
        "\n",
        "|Class|Convergence Speed| Convergence Quality|\n",
        "|---|---|---|\n",
        "|**SGD**|ðŸŸ©â¬œâ¬œ|ðŸŸ©ðŸŸ©ðŸŸ©|\n",
        "|**SGD**(`momentum`=..)|ðŸŸ©ðŸŸ©â¬œ|ðŸŸ©ðŸŸ©ðŸŸ©|\n",
        "|**SGD**(`momentum`=..., `nesterov=True`)|ðŸŸ©ðŸŸ©â¬œ|ðŸŸ©ðŸŸ©ðŸŸ©|\n",
        "|**AdaGrad**|ðŸŸ©ðŸŸ©ðŸŸ©|ðŸŸ©â¬œâ¬œ(Stops too early)|\n",
        "|**RMSProp**|ðŸŸ©ðŸŸ©ðŸŸ©|ðŸŸ©ðŸŸ©â¬œ or ðŸŸ©ðŸŸ©ðŸŸ©|\n",
        "|**Adam**|ðŸŸ©ðŸŸ©ðŸŸ©|ðŸŸ©ðŸŸ©â¬œ or ðŸŸ©ðŸŸ©ðŸŸ©|\n",
        "|**Nadam**|ðŸŸ©ðŸŸ©ðŸŸ©|ðŸŸ©ðŸŸ©â¬œ or ðŸŸ©ðŸŸ©ðŸŸ©|\n",
        "|**AdaMax**|ðŸŸ©ðŸŸ©ðŸŸ©|ðŸŸ©ðŸŸ©â¬œ or ðŸŸ©ðŸŸ©ðŸŸ©|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUv-2B1rAzl"
      },
      "source": [
        "## Learning Rate Scheduling\n",
        "There are many different strategies to reduce the learning rate during training (instead of keeping it constant 0.001). These strategies are called *learning schedules*. Some of the most commonly used learning schedules are:\n",
        "- ***Power Scheduling***\n",
        "\n",
        "  Set the learning rate to a function of the <mark>iteration number $t$.</mark>\n",
        "  $$\\eta(t) = \\frac{\\eta_0}{(1+t/s)^c} $$\n",
        "  where:\n",
        "  - $\\eta_0$ represent initial learning rate\n",
        "  - $c$ power (typically set to 1).\n",
        "  - $s$ represents steps.\n",
        "  - $t$ is iteration number\n",
        "\n",
        "  After s steps, the learning rate drops to $\\eta_0/2$, after $s$ more steps it is down to $\\eta_0/ 3$, then again after s steps to $\\eta_0/4$, and keeps decreasing.\n",
        "\n",
        "  First drops quickly and then slowly and slowly.\n",
        "- ***Exponential Scheduling***\n",
        "\n",
        "  $$\\eta(t) = \\eta_0\\; 0.1^{t/s}$$\n",
        "  The learning rate will gradually drop by a factor of 10 every $s$ steps.\n",
        "\n",
        "- ***Piecewise constant scheduling***\n",
        "\n",
        "  Use a constant learning rate for a number of epochs then a smaller learning rate for another number of epochs, and so on.\n",
        "\n",
        "- ***Performance Scheduling***\n",
        "\n",
        "  Measure the validation error every $N$ steps (just like early stopping), and reduce the learning rate by a factor of $\\lambda$ when the error stops dropping.\n",
        "\n",
        "- ***1cycle Scheduling***\n",
        "\n",
        "  Starts by increasing the initail learning rate $\\eta_0$ growing linearly up to $\\eta_1$ halfway through training. Then it decreases the learning rate down to $\\eta_0$ aain during the second half of training, finishing the last few epochs by dropping the rate down by several orders of magnitude (still linearly).\n",
        "\n",
        "  The maximum learning rate $\\eta_1$ is choosen using the same approach we used to find the optimal learning rate, and the initial learning rate $\\eta_0$ is choosen to be roughly 10 times lower. \n",
        "\n",
        "**Implementing power scheduling in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQE7Jn2kLySC"
      },
      "source": [
        "# just set the decay hyperparameter when creating optimizer\n",
        "optimizer = keras.optimizer.SGD(lr=0.01, decay=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L52Uv53MXoLT"
      },
      "source": [
        "The `decay` is the inverse of $s$ (the number of steps it takes to divide the learning rate by one more unit), and keras assumes that $c$ is equal to 1.\n",
        "\n",
        "**Implementing Exponential Scheduling and Piecewise scheduling in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVVgkQyEYQah"
      },
      "source": [
        "def exponential_decay_fn(epoch):\n",
        "  \"\"\"\n",
        "  Returns learing rate using exponential scheduling.\n",
        "  Takes current epoch count(t) and drop by a factor of 10 every 20 steps (s).\n",
        "  \"\"\"\n",
        "  return 0.01 * 0.1**(epoch / 20)\n",
        "\n",
        "def exponential_decay(lr_0, s):\n",
        "  def exponential_decay_fn(epoch):\n",
        "    return lr_0 * 0.1**(epoch/ s)\n",
        "  return exponetial_decay_fn\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr_0=0.01, s=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNJ7x3wgaCsu"
      },
      "source": [
        "Next, create a `LearningRateScheduler` callback, giving it the schedule function, and pass this callback to the `fit()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iVguAF8aSvV"
      },
      "source": [
        "lr_scheduler = keras.callback.LearningRateScheduler(exponential_decay_fn)\n",
        "history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2sOu2wpgPEJ"
      },
      "source": [
        "The schedule function can optionally take the current learning rate as a second argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si10kh7sgVaF"
      },
      "source": [
        "def exponential_decay_fn(epoch, lr):\n",
        "  \"\"\"\n",
        "  Multiples the previous learning rate by 0.1^1/20,\n",
        "  which result in the same exponential decay\n",
        "  \"\"\"\n",
        "  return lr*0.1**(1/20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yuAmJQgiTmA"
      },
      "source": [
        "When you save a model, the optimizer and its learning rate get saved along with it. This means that with this new shedule function (the one just above), you could just load a trained model and continue training where it left off.\n",
        "\n",
        "However, things are not so simple if your schedule function uses the epoch argument (like the previous 2 functions), since the <mark> the epoch does not get saved, and it gets reset to 0 every time you call the `fit()` method.\n",
        "\n",
        "One solution is to manually set the `fit()` methods `initial_epoch` argument so the `epoch` starts at the right value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrEZO9DfjVYt"
      },
      "source": [
        "**Implementing Piecewise contant scheduling in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnSbS8CejUz5"
      },
      "source": [
        "def priecewise_constant_fn(epoch):\n",
        "  if epoch < 5:\n",
        "    return 0.01\n",
        "  elif epoch < 15:\n",
        "    return 0.005\n",
        "  else:\n",
        "    return 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1tkPcODj-i0"
      },
      "source": [
        "**Implementing Performance Scheduling in Keras**\n",
        "Use the `ReduceLROnPlateau`callback. The following callback will multiply the learning rate by 0.5 whenever the best validation loss does not improve for five consecutive epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf2aIrW9kEoK"
      },
      "source": [
        "lr_scheduler = keras.callbacks.ReduceLROnPlateru(factor=0.5, patience=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6BV37OxleYx"
      },
      "source": [
        "`tf.keras` offers an alternative way to implement learning rate scehduling: \n",
        "\n",
        "**Define the learning rate using one of the schedules availaible in `keras.optimizers.schedules`, then pass the learning this learning rate to any optimizer.**\n",
        "\n",
        "This <mark>updates the learning rate at each step</mark> rather than at each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDQ4zffxSiPk"
      },
      "source": [
        "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size=32)\n",
        "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRQmph8gTx_F"
      },
      "source": [
        "<mark>When you save the model, the learning rate and its schedule (including its state) get saved as well.</mark>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftuvEC-0VWCz"
      },
      "source": [
        "# Avoiding Overfitting Through Regularization\n",
        "With the Neural Network's immense amount of parameters, they can fit a huge variety of complete datasets and sometimes even overfitting.\n",
        "\n",
        "We have already seen one such great technique to avoid overfitting, *early  stopping*. In this section we will examine other popular regularization techniques for neural networks:\n",
        "- [$\\ell_1$ and $\\ell_2$ regualrization](#l1-l2)\n",
        "- [Dropout Regularization](#dropout)\n",
        "- [Max-Norm regularization](#max-norm) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NO9-li0eJK2"
      },
      "source": [
        "## <a name=\"l1-l2\"></a>$\\ell_1$ and $\\ell_2$ regularization\n",
        "You can use $\\ell_2$ regularization to constrain a neura network's connection weights, and/or $\\ell_1$ regularization if you want a sparse model.\n",
        "\n",
        "**Implementing $\\ell_2$ regularization to a keras layer's connection weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGPwDZiNTxq1"
      },
      "source": [
        "layer = keras.layers.Dense(100, activation=\"elu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmOzBsyNg4Fe"
      },
      "source": [
        "The `l2()` functions returns a regularizer that will be called at each step during training to compute the regularization loss. This is then added to the final loss.\n",
        "\n",
        "Since, we probably want to apply the same operation; regularization, and activation function, we will find ourselves repeating the same arguments, making the code ugly and error-prone. \n",
        "\n",
        "To avoid this use Python's **`functools.partial()`** function, which lets us create a thin wrapper for any callable, with some default argument values:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FKCxLDsmBi5"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "                           activation='elu',\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "      keras.layers.Flatten(input_shape=(28, 28)),\n",
        "      RegularizedDense(300),\n",
        "      RegularizedDense(100),\n",
        "      RegularizedDense(10, activation=\"softmax\",\n",
        "                       kernel_initializer=\"glorot_uniform\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWi3ICj8nhVv"
      },
      "source": [
        "## <a name=\"dropout\"></a>Dropout\n",
        "At every training step, every neuron (includin the input neurons, but always **excluding the output neurons**) has a probability $p$ of being temporarily \"dropped out\", meaning it will be entirely ignored during this training step, but it may be active during the next step.\n",
        "\n",
        "This hyperparameter <mark>$p$ is called ***dropout rate***.</mark>\n",
        "\n",
        "Typically set around 10% and 50%.\n",
        "- close to 20-30% in recurrent nets.\n",
        "- close to 40-50% in convulational neural networks.\n",
        "\n",
        "<mark>Neurons trained with dropout cannot co-adapt with their neighbouring neurons; **they have to be as useful as possible on their own**.</mark>\n",
        "\n",
        ">ðŸŸ¢In practice, you can usually apply dropout only to the neurons in the top one to three layers (excluding the output layer) instead of every layer in the network.\n",
        "\n",
        "<mark>There is a small catch though. Suppose $p = 50%$, in which case during testing a neuron would be connected to twice as many input neurons as it would be (on average) during training.</mark> To compensate for this, we need to multiply each neuron's input connection weights by 0.5 after training. If we dont't each neuron will et a total input signal roughly twice as large as what the network was trained on and will be unlikely to perform well. \n",
        "\n",
        "**We need to multiply each input connection weight by the <mark>_keep probability_</mark> ($1-p$) after training**.\n",
        "\n",
        "Alternatively, we can divide each neuron's output by th keep probability during training.\n",
        "\n",
        "**Implementing dropout in Keras**\n",
        "- Use `keras.layers.Dropout`.\n",
        "- After training, just passes the inputs to next layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWEDSDt24EUg"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH6-BxJ58iFj"
      },
      "source": [
        ">ðŸŸ  <mark>Since dropout is only active during training, comparing the training loss and the validation loss can be misleading.</mark> In particular, a model may be overfitting the training set and yet have similar training and validation loss. So **make sure to evaluate the training loss without dropout (e.g., after training)**\n",
        "\n",
        "Decrease dropout if the model underfits, otherwise increase the dropout rate when the model overfits.\n",
        "\n",
        ">ðŸŸ¢ <mark>If we want to regularize a self-normalizing netowork based on the SELU activation function</mark>, use ***alpha dropout***, which preserves the mean and standard deviation, where otherwise regular regularization would break self-normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7typlms3C9UE"
      },
      "source": [
        "## Monte Carlo (MC) dropout\n",
        "In 2016, a paper by Yarin Gal and Zoubin Ghahramani added  few more good reasons to use dropout:\n",
        "- The paper established a profound connection between dropout network and approximate Bayesian inference (i.e., <mark>a dropout network is mathematically equivalent to approximate Bayesian inference in a specific type of probablistic model called a ***Deep Gaussian Process***.</mark>\n",
        "- They introduced *MC Dropout* which can boost performance of trained dropout network without having to retrain or modify it.\n",
        "\n",
        " \n",
        "**Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate**\n",
        "\n",
        "**Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHxr66_5Nv68"
      },
      "source": [
        "y_probas = np.stack([model(X_test_scaled, training=True)\n",
        "                    for sample in range(100)])\n",
        "y_proba = y_probas.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jv_LINZdoL-"
      },
      "source": [
        ">ðŸ”µ The number of Monte Carlo samples you use (previoud example, 100) is a hyperparameter you can tweak. <mark>The higher it is, the more accureate the predictions and their unceratainity estimates will be</mark>. Which also might increase the inference time, so find a right trade-off depending on the application of use.\n",
        "\n",
        "You should replace the `Dropout` layer with the following **`MCDropout`** class, if the model contains other layers that behave in a special way during training (such as `BatchNormalization`) instead of forcing training mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmpilIYwfdtE"
      },
      "source": [
        "class MCDropout(keras.layers.Dropout):\n",
        "  def call(self, inputs):\n",
        "    return super().call(inputs, training=True)\n",
        "\n",
        "\n",
        "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
        "  # When you have self-normalizing network \n",
        "  # based on the SELU activation function\n",
        "  def call(self, inputs):\n",
        "    return super().call(inputs, training=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOKwqI7TQAVQ"
      },
      "source": [
        "<mark>And yeah, if you are creating a model from scratch, it's just a matter of using `MCDropout` rather than `Dropout`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNVw9Ykkhz36"
      },
      "source": [
        "## <a name=\"max-norm\"></a> Max-Norm Regularization\n",
        "<mark>For each neuron, it constrains the weights $\\textbf{w}$ of the incoming connection such that $\\lvert\\lvert\\;{\\textbf{w}}\\;\\rvert\\rvert_2 \\leqslant r$, where $r$ is the max-norm hyperparameter and $\\lvert\\lvert \\;.\\; \\rvert\\rvert_2$ is the $\\ell_2$ norm.</mark>\n",
        "\n",
        "Max-norm regularization does not add a regularization loss term to the overall loss function. Instead, it is typically implemented by computing $\\lvert\\lvert\\;\\textbf{w}\\;\\rvert\\rvert_2$ after each training step and rescalling $\\textbf{w}$ if needed ($\\textbf{w}\\leftarrow\\frac{r}{\\lvert\\lvert\\;\\textbf{w}\\;\\rvert\\rvert_2}$).\n",
        "\n",
        "**Reducing $r$ increases the amount of regularization and helps reduce overfitting.**\n",
        "\n",
        "**Implementing in Keras**\n",
        "- set the `kernel_constraint` argument of each hidden layer to `max_norm()` constraint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UQ_qX60VfOc"
      },
      "source": [
        "keras.layers.Dense(100, activation=\"elu\",\n",
        "                   kernel_initializer='he_normal',\n",
        "                   kernel_contraint=keras.contraints.max_norm(1.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyhSbkes8Hff"
      },
      "source": [
        "We can also constrain the bias terms by setting the `bias_constraint` argument. \n",
        "\n",
        "The `max_norm()` function has an `axis` argument that defaults to $0$ meaning that the max-norm constraint will apply independently to each neurons's weight vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIGmQ0XBPLBy"
      },
      "source": [
        "# Summary and Practical Guidelines\n",
        "*Table 11-3. Default DNN configuration (not to be considered as hard & fast rule)*\n",
        "\n",
        "|**Hyperparameter**|**Default value**|\n",
        "|---|---|\n",
        "|**Kerenl initializer**|He initialization|\n",
        "|**Activation function**| ELU|\n",
        "|**Normalization**| None if shallow; Batch Norm if Deep|\n",
        "|**Regularization**| Early stopping (+$\\ell_2$ reg if needed)|\n",
        "|**Optimizer**| Momentum optimization (or RMSProp or Nadam)|\n",
        "|**Learing rate schedule**| 1cycle|\n",
        "\n",
        "If the network is a simple stack of dense layers, then it can self-normalize, and you should use the configuration in Table 11-4 instead.\n",
        "\n",
        "*Table 11-4. DNN configuration for a self-normalizing net*\n",
        "\n",
        "|**Hyperparameter**|**Default Value**|\n",
        "|---|---|\n",
        "|**Kerenl initializer**|LeCun initialization|\n",
        "|**Activation function**| SELU|\n",
        "|**Normalization**| None (Self-Normalization) |\n",
        "|**Regularization**| Alpha Dropout if needed|\n",
        "|**Optimizer**| Momentum optimization (or RMSProp or Nadam)|\n",
        "|**Learing rate schedule**| 1cycle |\n",
        "\n",
        "**Don't forget to normalize the input features.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb8U7Ni0PKDP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}