{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Ch 7 Ensemble Learning and Random Forests.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwhKZo1I2OrO"
      },
      "source": [
        "# Chapter 7: Ensemble Learning and Random Forests\n",
        "This work is partialy combined text and code from the book [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) is only supposed to be used as reference and is recommended to follow along with a copy of the Book puchased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kAapTc32NQp"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB7pEeV72NQv",
        "outputId": "28582fcd-d804-420f-df5c-a562126c0d83"
      },
      "source": [
        "X, y= make_moons(n_samples=200, noise=0.15)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rndf_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "                estimators=[('lr', log_clf),\n",
        "                            ('rf', rndf_clf),\n",
        "                            ('svc', svm_clf)],\n",
        "                voting='hard'\n",
        ")\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
              "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dzCXgoj2NQx",
        "outputId": "60425fa0-474f-4399-d194-e7f6db5654f6"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for clf in (log_clf, rndf_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.88\n",
            "RandomForestClassifier 0.94\n",
            "SVC 0.98\n",
            "VotingClassifier 0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNdyFRrC2NQy"
      },
      "source": [
        "# Bagging and Pasting in Scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8dmt8Ny2NQy"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "            DecisionTreeClassifier(),\n",
        "            n_estimators=500,\n",
        "            max_samples=100,\n",
        "            bootstrap=True,          # Pasting instead; set False\n",
        "            n_jobs=-1,\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxFddFjF2NQz"
      },
      "source": [
        "# Out-of-Bag Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrArWr_22NQz",
        "outputId": "3bf8443a-bf09-4339-cf8c-1ef99d7b3f98"
      },
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "            DecisionTreeClassifier(),\n",
        "            n_estimators=500,\n",
        "            max_samples=100,\n",
        "            bootstrap=True,\n",
        "            n_jobs=-1,\n",
        "            oob_score=True,\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_                  # Likely to achieve 94.6% accuracy on Test Set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9266666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvs52Ubp2NQz",
        "outputId": "9178ddb8-0e31-48be-b01e-924781980bd6"
      },
      "source": [
        "# Let's Verify\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.94"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLym11rF2NQ0",
        "outputId": "f4d9f497-b3b5-4c55-d0e1-8c5074a5270c"
      },
      "source": [
        "# Shape is (150, 2) because X_test has 150 instances.\n",
        "bag_clf.oob_decision_function_[:5] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99609375, 0.00390625],\n",
              "       [0.07509881, 0.92490119],\n",
              "       [0.99215686, 0.00784314],\n",
              "       [0.96538462, 0.03461538],\n",
              "       [0.73809524, 0.26190476]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F8oBA5l2NQ0"
      },
      "source": [
        "# Random Patches and Random Subspaces\n",
        "\n",
        "1. **Random Patches**: Sampling both training instances and features.\n",
        "2. **Random Subspaces**: \n",
        "    \n",
        "    keeping all training instances (*`bootstrap = False`* and *`max_samples = 1.0`*)\n",
        "    \n",
        "    ***BUT*** \n",
        "    \n",
        "    sampling features (*`bootstrap_features = True`* and *`max_features < 1.0`*)\n",
        "    \n",
        "Sampling feature result in even more predictor diversity.\n",
        "- Trading a bit more bias for a lower variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL25sreA2NQ1"
      },
      "source": [
        "# Random Forests\n",
        "\n",
        "- Genrally trained via bagging method.\n",
        "- *`max_sample`* set to the size of training set.\n",
        "\n",
        "The following code uses all availabile CPU cores to train a Random Forest classifier with 500 trees (each limited to maximum of 16 leaf nodes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEJ8ZwaG2NQ1",
        "outputId": "f827aa02-7932-41b2-f416-4ec0d76c4e21"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred_rf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.94"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T242hq02NQ1"
      },
      "source": [
        "*RandomForestClassfier* has all the hyperparameters of a *DecisionTreeClassifier* and *BaggingeClassifier*.\n",
        "- Introduces extra randomness; Searches for very best feature among a random subset of feature.\n",
        "- Trades for higher bias for lower variance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmQ7r7Mr2NQ2"
      },
      "source": [
        "# the following BagClassifier is just as same as above RandomForestClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "            DecisionTreeClassifier(splitter='random', max_leaf_nodes=16),\n",
        "            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQvG-_PQ2NQ2"
      },
      "source": [
        "## Extra-trees | Extremely Randomized Trees Ensemble\n",
        "\n",
        "Using **random thresholds for each feature** rather than searching for the best possible thresholds (like Decsion Trees do)\n",
        "\n",
        "Use `ExtraTreesClassifier` class, `ExtraTreesRegressor`; both of their API identical to RandomForestClassifier/RandomForestRegressor\n",
        "- Trades more Bias for a lower variance\n",
        "- Much faster to train than regular Random Forests.\n",
        "\n",
        "  B/c finding the best possible threhold for each feature at every node is one the most time-consuming tasks of growing a tree\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CGY-uFo2NQ3"
      },
      "source": [
        "## Feature Importance\n",
        "Scikit-Learn measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all tree in the forest).\n",
        "\n",
        "Scikit-learn computes this score automatically for each feature after training, then it sacles the result so that the sum of all imporatances is equal to 1. \n",
        "- *`feature_importances_`* will provide the result.\n",
        "\n",
        "**Random Forests are very handy to get a quick understanding of what features actually matter**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyk9T7Wq2NQ3",
        "outputId": "60ab5983-495b-4af6-ac5b-855d020af980"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
        "rnd_clf.fit(iris['data'], iris['target'])\n",
        "\n",
        "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
        "    print(name, score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sepal length (cm) 0.08475828580501725\n",
            "sepal width (cm) 0.022786389573764637\n",
            "petal length (cm) 0.4270325182779528\n",
            "petal width (cm) 0.46542280634326527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpnVkr2h2NQ4"
      },
      "source": [
        "# Boosting\n",
        "\n",
        "**Refers to any ensemble method that can combine several weak learners into a strong learner.**\n",
        "- Train predictors sequentially, each trying to correct its predecessor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNCKTP3E2NQ4"
      },
      "source": [
        "## AdaBoost\n",
        "**`AdaBoostClassifier`**\n",
        "\n",
        "New predictors focuses more and more on the hard cases, by paying more attention to the training instances the predecessor underfitted.\n",
        "\n",
        "- Alogrithm first trains a base classifier, uses it to make prediction, on the training set.\n",
        "- Algorithm then increases the relative weight of miscalssified training instances.\n",
        "- Train second classifier, using updated weights, and again makes predictions, update instance weights, and so on.\n",
        "\n",
        "*AdaBoost adds predictors to the ensemble, gradually making it better*; instead of tweaking single predictors's parameters to minimize a cost function, like Gradient Descent.\n",
        "\n",
        "**Can't be parallelized, since each predictor can only be trained after the previous predictor has been trained and evaluated**\n",
        "\n",
        "**IF OVERFITTING:**\n",
        "1. Reduce the number of estimators\n",
        "2. More strongly regularize the base estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv3i8rDk2NQ5",
        "outputId": "d5d2e0a3-c8d6-407f-e5f0-b2444305e01d"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), \n",
        "    n_estimators=500,\n",
        "    algorithm=\"SAMME.R\",\n",
        "    learning_rate=0.5)\n",
        "ada_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   learning_rate=0.5, n_estimators=500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQKs30qH2NQ6"
      },
      "source": [
        "## Gradient Boosting\n",
        "**`GradientBoostingRegressor`** much like `RandomForestRegressor`, has hyperparameters to control the growth of Decision Trees, as well as hyperparameter to control the ensemble training (such as *`n_estimaotors`*) \n",
        "\n",
        "Just like AdaBoost, but instead of tweaking the instance weights at every instance, this method \n",
        "\n",
        "### Tries to fit the new predictor to the *residual errors* made by the previous predictor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "d2OoVmqE2NQ6",
        "outputId": "644d3473-25ad-4e1a-e219-ae2f0f806c68"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg1.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmkamtPa2NQ6",
        "outputId": "96b8439f-ad08-4ae7-a9ac-786b9784d758"
      },
      "source": [
        "# Now we'll train a second DecisionTreeRegressor on the residual errors made by the first predictor.\n",
        "y2 = y - tree_reg1.predict(X)\n",
        "\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg2.fit(X, y2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2a4EYkj2NQ7",
        "outputId": "a2cdd0e7-d44b-458f-f072-fb448995436e"
      },
      "source": [
        "# Now we'll train a third DecisonTreeRegeressor on the residual errors made by the second predictor.\n",
        "y3 = y2 - tree_reg2.predict(X)\n",
        "\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg3.fit(X, y3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6MWSydA2NQ7"
      },
      "source": [
        "- Now we have ensemble of three trees.\n",
        "- **Predictions will be made by simply adding up the the predictions of all the trees.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugOWMiPL2NQ7"
      },
      "source": [
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
        "y_pred "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxuHoeZO2NQ7"
      },
      "source": [
        "### Using Sklearn class\n",
        "\n",
        "*`learning_rate`* hyperparameter scales the contribution of each tree.\n",
        "\n",
        "- If set to low value (like `0.1`) you'll need more trees in the ensemble to fit the training set.\n",
        "\n",
        "  **But will generalize better**. This is a regularization technique called *shrienkage*.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXngHrN32NQ8",
        "outputId": "c9c6dd0f-a10d-4b56-8d2a-67631ce32d15"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
        "gbrt.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WS-2O8q2NQ8"
      },
      "source": [
        "### In order to find optimal number of trees, use **Early Stopping**.\n",
        "The below code trains 120 trees first and then looks back for the number of tree at which the validation error (mean_squared_error) was least.\n",
        "\n",
        "`staged_predict()` method returns an iterator over the prediction made by the ensemble at each stage of training (like ensemble with one tree, two tree, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB3eg_fl2NQ8"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
        "best_number_of_estimators = np.argmin(errors) + 1\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=best_number_of_estimators)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2H_--Ra2NQ8"
      },
      "source": [
        "#### Actually stopping early\n",
        "By setting *`warm_start`*`=True` makes Sckit-learn keep existing trees when `fit()` method is called, allowing **incremental training**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fFRhqVT2NQ8"
      },
      "source": [
        "# Stops training if the validation error does not improve for five iterations in a row.\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
        "\n",
        "min_val_error = float('inf')\n",
        "error_going_up = 0\n",
        "\n",
        "for n_estimators in range(1, 120):\n",
        "    gbrt.n_estimators = n_estimators\n",
        "    gbrt.fit(X_train, y_train)\n",
        "    val_error = mean_squared_error(y_val, gbrt.predict(X_val))\n",
        "    \n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error\n",
        "        error_going_up = 0\n",
        "    else:\n",
        "        error_going_up += 1\n",
        "        if error_going_up == 5:\n",
        "            break             # EARLY STOPPING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObSgkJWY2NQ9"
      },
      "source": [
        "### Stocashastic Gradient Boosting\n",
        "*`subsample`* hyperparameter specifies the fraction of training instances to be used for training each tree.\n",
        "\n",
        "- Trades a higher bias for lower variance.\n",
        "- Speeds up training considerably.\n",
        "- *`subsample`*`=0.25`, then each tree is trained on 25% of the training instances, selected RANDOMLY."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBdQRj7k2NQ9",
        "outputId": "d1181375-95c6-4261-9153-b6f716f990cf"
      },
      "source": [
        "import xgboost\n",
        "\n",
        "xgb_reg = xgboost.XGBRegressor()\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_val)[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'xgboost'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-44-5e4aaaf9815e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mxgb_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mxgb_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjhxxPls2NQ9"
      },
      "source": [
        "# XGBoost also offers several nice features, such as automatically taking care of early stopping\n",
        "\n",
        "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
        "xgb_reg.predict(X_val)[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUeX9w6r2NQ9"
      },
      "source": [
        "# Stacking | Stacked Generalization\n",
        "Instead of using a trivial function (like hard voting) to perform the aggregate the predictions of all predictors,\n",
        "\n",
        "### Train a model to perform this aggregation itself.\n",
        "- To train a blender, a common approach is to use a hold-out set.\n",
        "- Split a training set into two, first for training predictors in first layer and second (Hold-out).\n",
        "- The first layer predictors are made to make predictions on the hold-out set.\n",
        "- each predictors will output its predictions and this becomes the training set for the blender.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3y5c50k2NQ-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}