{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch 19 Training and Deploying TensorFlow  Models at Scale.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8JALaY6nSG2E06GkYprpQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhadse/colab_repo/blob/master/homl/Ch%2019%20Training%20and%20Deploying%20TensorFlow%20Models%20at%20Scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdahIb3urD9u"
      },
      "source": [
        "# Chapter 19: Training and Deploying TensorFlow  Models at Scale\n",
        "This work is partialy combined text and code from the book [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) is only supposed to be used as reference and is recommended to follow along with a copy of the Book purchased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxf7k7R2u8ZC"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXbDi0UUsOw3"
      },
      "source": [
        "#Serving a TensorFlow Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4SHkq1RsZoK"
      },
      "source": [
        "## Using TensorFlow Serving\n",
        "TF Serving is a very efficient, model server written in C++ which can serve multiple model versions and even autometically and gracefully switch to newer version when found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaUM9GxTs9XB"
      },
      "source": [
        "### Exporting SavedModels\n",
        "To use TF Serving we first need to export our models to *SavedModel format* using `tf.saved_model.save()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf4av6pIqZPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f952cc-d2a4-421e-dff1-d115fc8f7f2e"
      },
      "source": [
        "\"\"\"\n",
        "  First let's create the model\n",
        "\"\"\"\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255.\n",
        "X_test = X_test[..., np.newaxis].astype(np.float32) / 255.\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_new = X_test[:3]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX_TJc9CB9Er"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tJvBh2g1Rg-",
        "outputId": "ac1ed4d8-c268-4149-c6f2-7dd1fe800b3e"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 7s 2ms/step - loss: 0.6394 - accuracy: 0.8437 - val_loss: 0.3528 - val_accuracy: 0.9056\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3363 - accuracy: 0.9065 - val_loss: 0.2865 - val_accuracy: 0.9196\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2863 - accuracy: 0.9200 - val_loss: 0.2510 - val_accuracy: 0.9308\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2549 - accuracy: 0.9291 - val_loss: 0.2270 - val_accuracy: 0.9358\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2314 - accuracy: 0.9357 - val_loss: 0.2079 - val_accuracy: 0.9434\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2124 - accuracy: 0.9409 - val_loss: 0.1931 - val_accuracy: 0.9480\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1962 - accuracy: 0.9457 - val_loss: 0.1799 - val_accuracy: 0.9514\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1827 - accuracy: 0.9489 - val_loss: 0.1683 - val_accuracy: 0.9566\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1711 - accuracy: 0.9522 - val_loss: 0.1601 - val_accuracy: 0.9582\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1607 - accuracy: 0.9554 - val_loss: 0.1525 - val_accuracy: 0.9602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f01242c4750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmLN-lEB73_t",
        "outputId": "43c52d81-d6e0-4521-a68f-ccfb97ac7754"
      },
      "source": [
        "np.round(model.predict(X_new), 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.98, 0.02, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.98, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANZ6_qNL1hpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed92fb4c-907d-4124-845f-b45142884399"
      },
      "source": [
        "model_version = \"0001\"\n",
        "model_name = \"the_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "tf.saved_model.save(model, model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: the_mnist_model/0001/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbC1sF6sQP3"
      },
      "source": [
        "The SavedModel directory has:\n",
        "- `saved_model.pb` representing the computation graph.\n",
        "- *variables* subdirectory contaning the variables values.\n",
        "- *assets* subdirectory containing additional data, such as vocabulary  files,. etc.\n",
        "\n",
        "To load this model instead of using `tf.keras.models.load_model` use `tf.saved_model.load()` and set the `training` to be `False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJdZPVbt9LAZ",
        "outputId": "8120b635-be31-455c-d8f3-604f5068db95"
      },
      "source": [
        "!saved_model_cli show --dir {model_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel contains the following tag-sets:\n",
            "serve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jHZs6K__I_s"
      },
      "source": [
        "A SavedModel contains one or more *metagraphs* each identified by a set of tages.\n",
        "\n",
        "<mark>A metagraph is a computation graph plus  some function signature definations (including their input and output names, types, and shapes).</mark>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNyybfx_9rUH",
        "outputId": "5d729388-6b2d-4593-daee-b03bb5e0c3e4"
      },
      "source": [
        "!saved_model_cli show --dir {model_path} --tag_set serve"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
            "SignatureDef key: \"__saved_model_init_op\"\n",
            "SignatureDef key: \"serving_default\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiSyXYu8rC3F",
        "outputId": "a9f7da1d-7623-4256-b51b-a8d9cbe82077"
      },
      "source": [
        "!saved_model_cli show --dir {model_path} --tag_set serve\\\n",
        "                      --signature_def serving_default"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['flatten_2_input'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 28, 28, 1)\n",
            "      name: serving_default_flatten_2_input:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['dense_5'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 10)\n",
            "      name: StatefulPartitionedCall:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA5yyUsftss5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4881f9a1-4c27-44a3-8f52-67daf8b15d4b"
      },
      "source": [
        "!saved_model_cli show --dir {model_path} --all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['flatten_2_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 28, 28, 1)\n",
            "        name: serving_default_flatten_2_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['dense_5'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 10)\n",
            "        name: StatefulPartitionedCall:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 11:36:01.354573 140139511453568 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "\n",
            "Defined Functions:\n",
            "  Function Name: '__call__'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          flatten_2_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'flatten_2_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          flatten_2_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'flatten_2_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "\n",
            "  Function Name: '_default_save_signature'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          flatten_2_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'flatten_2_input')\n",
            "\n",
            "  Function Name: 'call_and_return_all_conditional_losses'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          flatten_2_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'flatten_2_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          flatten_2_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'flatten_2_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzN5eRMIDxqu"
      },
      "source": [
        "Let's write the new instances to a `npy` file so we can pass them easily to our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_54hncQxDlpi",
        "outputId": "40787dfb-95c2-4a52-e76f-b2e91d875260"
      },
      "source": [
        "np.save(\"my_mnist_tests.npy\", X_new)\n",
        "\n",
        "input_name = model.input_names[0]\n",
        "input_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'flatten_2_input'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySJl1NMMD3_C",
        "outputId": "02b9f217-803a-4ee4-c64b-2d4f507c4ae4"
      },
      "source": [
        "!saved_model_cli run --dir {model_path} --tag_set serve \\\n",
        "                     --signature_def serving_default \\\n",
        "                     --inputs {input_name}=\"my_mnist_tests.npy\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19 11:40:40.907974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-06-19 11:40:40.918655: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-06-19 11:40:40.918711: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (004b804ae975): /proc/driver/nvidia/version does not exist\n",
            "2021-06-19 11:40:40.927808: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2021-06-19 11:40:40.928141: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562fffb72a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-06-19 11:40:40.928180: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 11:40:40.934154 139733596084096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/tools/saved_model_cli.py:420: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "Result for output key dense_5:\n",
            "[[1.1348874e-04 3.4772970e-07 6.5677706e-04 2.6318526e-03 2.2479003e-06\n",
            "  4.3850094e-05 5.6311748e-08 9.9602258e-01 2.4540455e-05 5.0423446e-04]\n",
            " [4.2043632e-04 6.4254651e-05 9.8750740e-01 8.0940463e-03 3.2317416e-08\n",
            "  5.3414190e-04 2.5793216e-03 1.1070003e-08 8.0020179e-04 1.6184826e-08]\n",
            " [1.9224688e-05 9.7866690e-01 7.8524482e-03 1.9809359e-03 5.7935639e-04\n",
            "  1.1971863e-03 1.0196721e-03 6.2796283e-03 2.1492112e-03 2.5542409e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkYOBc-IEohC",
        "outputId": "6a3531a5-fa1e-4a33-c1a2-af3ff487492e"
      },
      "source": [
        "np.round([[1.1348874e-04, 3.4772970e-07, 6.5677706e-04, 2.6318526e-03, 2.2479003e-06,\n",
        "           4.3850094e-05, 5.6311748e-08, 9.9602258e-01, 2.4540455e-05, 5.0423446e-04],\n",
        "          [4.2043632e-04, 6.4254651e-05, 9.8750740e-01, 8.0940463e-03, 3.2317416e-08,\n",
        "           5.3414190e-04, 2.5793216e-03, 1.1070003e-08, 8.0020179e-04, 1.6184826e-08],\n",
        "          [1.9224688e-05, 9.7866690e-01, 7.8524482e-03, 1.9809359e-03, 5.7935639e-04,\n",
        "           1.1971863e-03, 1.0196721e-03, 6.2796283e-03, 2.1492112e-03, 2.5542409e-04]], 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDJp2g6XIl5m"
      },
      "source": [
        "### Installing TensorFlow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lU07xlLI7o9"
      },
      "source": [
        "First set up the repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRZR1J0956mt"
      },
      "source": [
        "Install [Docker](https://docs.docker.com/install/) if you don't have it already. Then run:\n",
        "\n",
        "```bash\n",
        "docker pull tensorflow/serving\n",
        "\n",
        "export ML_PATH=$HOME/ml # or wherever this project is\n",
        "docker run -it --rm -p 8500:8500 -p 8501:8501 \\\n",
        "   -v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\" \\\n",
        "   -e MODEL_NAME=my_mnist_model \\\n",
        "   tensorflow/serving\n",
        "```\n",
        "Once you are finished using it, press Ctrl-C to shut down the server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsfp_aC656mu"
      },
      "source": [
        "Alternatively, if `tensorflow_model_server` is installed (e.g., if you are running this notebook in Colab), then the following 3 cells will start the server:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cJh2kQkRI2hY",
        "outputId": "09d465eb-2931-4b86-8a5a-e7e59c9f7893"
      },
      "source": [
        "os.environ[\"MODEL_DIR\"] = os.path.split(os.path.abspath(model_path))[0]\n",
        "os.environ[\"MODEL_DIR\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/the_mnist_model'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoSVkNDFNQrL",
        "outputId": "fca0468f-da85-434f-cb09-4c4eb7aa740d"
      },
      "source": [
        "%%bash --bg\n",
        "nohup tensorflow_model_server \\\n",
        "      --rest_api_port=8501 \\\n",
        "      --model_name=\"the_mnist_model\" \\\n",
        "      --model_base_path=\"${MODEL_DIR}\" > server.log 2>&1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 0 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ6KASfKPUE0",
        "outputId": "63b0747f-52ca-4151-f318-48573a342292"
      },
      "source": [
        "!sudo apt-get remove tensorflow-model-server"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package tensorflow-model-server\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyOQHIhMOXws",
        "outputId": "da3dd5c9-a67c-4c8c-da6c-9535be2157b7"
      },
      "source": [
        "!tail server.log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nohup: failed to run command 'tensorflow_model_server': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy0sosHbOove",
        "outputId": "9123f58d-c04c-4b78-d6c5-c418bf2c0d1e"
      },
      "source": [
        "!echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "!curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n",
            "/bin/bash: !curl: command not found\n",
            "gpg: no valid OpenPGP data found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXnfH4MBP9n5",
        "outputId": "7a0311dd-fa3e-430f-c59b-c31ac8c4cdf3"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install ca-certificates"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,012 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [1 InRelease gpgv 3,012 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rHit:2 https://download.docker.com/linux/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,012 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 3,012 B] [Connecting to archive.ubuntu.com] [3 InRelease 1\r                                                                               \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Err:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease\n",
            "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 544B7F63BF9E4D5F\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Reading package lists... Done\n",
            "W: GPG error: http://storage.googleapis.com/tensorflow-serving-apt stable InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 544B7F63BF9E4D5F\n",
            "E: The repository 'http://storage.googleapis.com/tensorflow-serving-apt stable InRelease' is not signed.\n",
            "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
            "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ca-certificates is already the newest version (20210119~18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8KZqrtDOa95",
        "outputId": "2931b4c5-4b8e-4cde-88ee-68f5032ee924"
      },
      "source": [
        "!sudo apt-get install tensorflow-model-server"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,012 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [1 InRelease gpgv 3,012 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rHit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [1 InRelease gpgv 3,012 B] [Connecting to archive.ubuntu.com] [Connected to \r                                                                               \rHit:3 https://download.docker.com/linux/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,012 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 3,012 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rErr:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease\n",
            "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 544B7F63BF9E4D5F\n",
            "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "W: GPG error: http://storage.googleapis.com/tensorflow-serving-apt stable InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 544B7F63BF9E4D5F\n",
            "E: The repository 'http://storage.googleapis.com/tensorflow-serving-apt stable InRelease' is not signed.\n",
            "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
            "N: See apt-secure(8) manpage for repository creation and user configuration details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9mg8lAIQyGz"
      },
      "source": [
        "### Querying TF Serving through the REST API\n",
        "First we'll create the query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeYfdHHnPor3"
      },
      "source": [
        "import json\n",
        "\n",
        "input_data_json = json.dumps({\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": X_new.tolist(),\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wG2rcfImeW_"
      },
      "source": [
        "input_data_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4bsFlGEmwk-"
      },
      "source": [
        "And then we can send the input data to TF Serving by sending an HTTP POST request."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvuXHZdqmta1"
      },
      "source": [
        "import requests\n",
        "\n",
        "SERVER_URL = \"http://localhost:8501/v1/models/the_mnist_model:predict\"\n",
        "response = requests.post(SERVER_URL, data=input_data_json)\n",
        "response.raise_for_status() # raise an exception in case of error\n",
        "response = response.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccn3Itm-ngY7"
      },
      "source": [
        "y_proba = np.array(response[\"predictions\"])\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOeBz27fnoFy"
      },
      "source": [
        "### Serving TF Serving through the gRPC API\n",
        "The gRPC API expects a serialized `PredictRequest` protocol buffer  as input and it outputs a serialized `PredictResponse` protocol buffer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms66dEAoo34E",
        "outputId": "1430154f-064d-47cb-b8f6-5d1fa6a1740a"
      },
      "source": [
        "!pip install tensorflow-serving-api"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-serving-api\n",
            "  Downloading https://files.pythonhosted.org/packages/27/a6/a534deae4086c0fef9a77537a4779bb5a9dd8841f8d347c509c96b342b2e/tensorflow_serving_api-2.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: grpcio<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api) (1.34.1)\n",
            "Requirement already satisfied: tensorflow<3,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api) (2.5.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api) (3.12.4)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<2,>=1.0->tensorflow-serving-api) (1.15.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (0.12.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (2.5.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (0.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (3.3.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.0->tensorflow-serving-api) (57.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.31.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.8.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.5.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (4.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (4.5.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (1.24.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<3,>=2.5.0->tensorflow-serving-api) (3.1.1)\n",
            "Installing collected packages: tensorflow-serving-api\n",
            "Successfully installed tensorflow-serving-api-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Q7yASUoHOD"
      },
      "source": [
        "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
        "\n",
        "request = PredictRequest()\n",
        "request.model_spec.name = model_name\n",
        "request.model_spec.signature_name = \"serving_default\"\n",
        "input_name = model.input_names[0]\n",
        "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTYyG1_2pTjl"
      },
      "source": [
        "Now we are ready to request the server and get the server. For this we need to install `grpcio` library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cb3IUXKouBJ",
        "outputId": "64a7f875-3af6-4624-e6fb-ad47828bc9b3"
      },
      "source": [
        "!pip install grpcio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.7/dist-packages (1.34.1)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16xEdgRbpgTK"
      },
      "source": [
        "import grpc\n",
        "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
        "\n",
        "channel = grpc.insecure_channel(\"localhost:8500\")\n",
        "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
        "response = predict_service.Predict(request, timeout=10.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA6xiKrxqJex"
      },
      "source": [
        "# convert the PredictReponse protocol buffer to a tensor\n",
        "output_name = model.output_names[0]\n",
        "output_proto = reponse.outputs[output_name]\n",
        "y_proba = tf.make_ndarray(output_proto)\n",
        "\n",
        "y_proba.numpy.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTud6yKqqoFL"
      },
      "source": [
        "### Deploying a new model version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tf99jY_qxY5",
        "outputId": "c6c8520e-4966-415d-f7eb-812a7a5415ec"
      },
      "source": [
        "model_version = \"0002\"\n",
        "model_name = \"the_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "tf.saved_model.save(model, model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: the_mnist_model/0002/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbITPPzT56nD"
      },
      "source": [
        "# Deploy the model to Google Cloud AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3bJ0dRn56nE"
      },
      "source": [
        "Follow the instructions in the book to deploy the model to Google Cloud AI Platform, download the service account's private key and save it to the `my_service_account_private_key.json` in the project directory. Also, update the `project_id`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aefXE25LrCjV"
      },
      "source": [
        "import googleapiclient.discovery\n",
        "\n",
        "project_id = \"onyx-smoke-242003\" # change this to your project ID\n",
        "model_id = \"the_mnist_model\"\n",
        "model_path = 'prjoject/{}/models/{}'\n",
        "ml_resource = googleapiclient.discovery.build(\"ml\", \"v1\").projects()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPqQlepx0NMX"
      },
      "source": [
        "def predict(X):\n",
        "  input_data_json = {\"signature_name\": \"serving_default\",\n",
        "                     \"instances\": X.tolist()}\n",
        "  request = ml_resource.predict(name=model_path, body=input_data_json)\n",
        "  response = request.execute()\n",
        "  if \"error\" in response:\n",
        "    raise RuntimeError(response[\"error\"])\n",
        "  return np.array([pred[output_name] for pred in response[\"predictions\"]])\n",
        "\n",
        "y_probas = predict(X_new)\n",
        "np.round(y_probas, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgAVgnXY3YwP"
      },
      "source": [
        "# Using GPUs to Speed Up Computations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEGy3dfA3f0F"
      },
      "source": [
        "## Getting Your Own GPU\n",
        "TensorFlow currently only supports Nvidia cards with CUDA (Compute Unified Device Architecture) library  compute capabilities. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcIgyPjt3fHL",
        "outputId": "994074f1-667c-48e5-f160-00950d3e5a3c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun 19 15:38:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0xYGDU74meq",
        "outputId": "0960f349-2268-4f1d-9fcc-0c175166265c"
      },
      "source": [
        "tf.config.list_physical_devices(\"GPU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4WPHVk3x4xZH",
        "outputId": "e75313cd-a3cb-425d-c3a9-729ea32aec3b"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETvRjFNe5EhQ"
      },
      "source": [
        "### Splitting the GPU into virtual devices \n",
        "Split the GPU to 4 virtual GPUs with 3777 GiB of RAM each. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM6sfQPg49PE"
      },
      "source": [
        "\"\"\"\n",
        "Do so before using TensorFlow\n",
        "\"\"\"\n",
        "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "\n",
        "tf.config.experimental.set_virtual_device_configuration(\n",
        "    physical_gpus[0],\n",
        "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3777),\n",
        "     tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3777),\n",
        "     tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3777),\n",
        "     tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3777),]\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruW9Yrk79d9L",
        "outputId": "698fa75f-217f-4a3f-d4da-8f1f164a73f1"
      },
      "source": [
        "tf.config.list_logical_devices(\"GPU\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[LogicalDevice(name='/device:GPU:0', device_type='GPU'),\n",
              " LogicalDevice(name='/device:GPU:1', device_type='GPU'),\n",
              " LogicalDevice(name='/device:GPU:2', device_type='GPU'),\n",
              " LogicalDevice(name='/device:GPU:3', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1whvMjh7P_D"
      },
      "source": [
        "## Managing the GPU RAM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mThzL0mr-ur3"
      },
      "source": [
        "### Assigning a GPU to single dedicated process\n",
        "If we have multiple GPU cards on our machine, a simple solution is to assign each of them to a single process.\n",
        "\n",
        "We can set `CUDA_VISIBLE_DEVICES` environment variable so that each process only sees the appropriate GPU card(s). Also set the `CUDA_DEVICE_ORDER` environment variable  to `PCI_BUS_ID` to ensure that each ID always refers to the same card."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnPGA79Q7HiA"
      },
      "source": [
        "!CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,1 python 3 program_1.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS_Pxs2J9w8_"
      },
      "source": [
        "!CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=2,3 python 3 program_2.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfoLNivO-Umf"
      },
      "source": [
        "### Tell TensorFlow to grab only specific amount of GPU RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQxLlrZf-5c8"
      },
      "source": [
        "\"\"\"\n",
        "Do immendiately after importing TF\n",
        "\"\"\"\n",
        "for gpu in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
        "  tf.config.experimental.set_virtual_device_configuration(\n",
        "      gpu,\n",
        "      [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)]\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fjj1cC4kvLD"
      },
      "source": [
        "### Tell TF to grab memory only when it needs it\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIfexSqJk5vB"
      },
      "source": [
        "for gpu in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
        "  tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSDBnZGp_k9q"
      },
      "source": [
        "### Placing Operations and Variables on Devices\n",
        "`tf.keras` and `tf.data` generally put heavy computation on the GPU, and the data preprocessing on the CPU and are able to good at doing that.\n",
        "\n",
        "But we can also force to place some operations and variables on each device, if we want more control.\n",
        "\n",
        "A tensor or variable's `device` attribute  tell where it is placed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdFO3SmM_hJM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "902a9f3c-a2cc-4498-d8bf-773fa2f35b44"
      },
      "source": [
        "a = tf.Variable(42.0)\n",
        "a.device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/job:localhost/replica:0/task:0/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XENrM9GNmcTY",
        "outputId": "ffb1694f-8340-4b9a-f7b4-e9048f054a1e"
      },
      "source": [
        "b = tf.Variable(42)\n",
        "b.device"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/job:localhost/replica:0/task:0/device:CPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAO3ZFWEmkq_"
      },
      "source": [
        "Use `device()` context to place an operation on a different device than the default one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NKScAANjmgrD",
        "outputId": "3497ee67-1871-444b-ba17-873dc1a5683c"
      },
      "source": [
        "with tf.device(\"/cpu:0\"):\n",
        "  c = tf.Variable(42.0)\n",
        "\n",
        "c.device"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/job:localhost/replica:0/task:0/device:CPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BFeGbWmnNP3"
      },
      "source": [
        "# Training Models Across Multiple Devices\n",
        "There are two main approaches to this:\n",
        "- *Model Parallelism*: the model is plit across the devices. Mostly of the time not doable.\n",
        "- *Data Parallelism*: the model is replicated across every device and each replica is trained on a subset of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_ngM_wHoGhK"
      },
      "source": [
        "## Data Parallelism\n",
        "The NN is replicated on every device and each training step is ran simulatneously on all replicas using different mini-batach for each. The gradients from the replicas are averaged and used for updating the model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s54mzQdoqN4"
      },
      "source": [
        "### Data Parallelism using the mirrored strategy\n",
        "- Mirrors all the model parameters across all the GPUs and always  apply the exact same parameter updates on every GPU. \n",
        "- Imposes synchronous weight updates across all GPUs. That means if there is a slow GPU all other \"fast\" GPUs have to wait till that finishes to get the average computed and for next training step to begin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_tL4l9dpfVz"
      },
      "source": [
        "### Data Parallelism with centralized parameters.\n",
        "- The model parameters are stored outside of the GPU devices performing the computation (the *workers*) and plaecs on one or more  CPU-only servers called *parameter servers*.\n",
        "- This approach allows either synchronous or asynchronours updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoJiHT2Mqjn8"
      },
      "source": [
        "### Synchronous updates\n",
        "The aggregator waits until all gradients are available before it computes the average gradients and passes them to the optimizer.\n",
        "\n",
        "The downside being the aggregator will have to wait for the slower replicas until it finish its training step. This problem can be eliminited by simpy ignorng the gradients from the slowest few replicas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE4qiFLK1xv3"
      },
      "source": [
        "### Asynchronous updates\n",
        "Whenever a replica has finished computing the gradients, it immediately uses them to update the model parameters. There is aggregation and no synchronization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtNJeRah2GKl"
      },
      "source": [
        "### Bandwidth saturation\n",
        "Whether we use synchronous or asynchronous updates, with data parallelism with centralized parameters or with mirrored strategy there always comes a pont where adding an extra GPU will not improve performance at all b/c the time spent moving the data into and out of GPU RAM will outweigh the speedup obtained by splitting the computation load.\n",
        "\n",
        "To reduce the saturation problem, we should \n",
        "- try to use fewer GPUs rather than plenty of weak GPUs. \n",
        "- Try droppingg the float precision from 32 Bits (`tf.float32`) to 16 bits (`tf.bfloat16`).\n",
        "- And if we are using centralized parameters, we should split the parameters acrross multiple parameters servers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cIzK5xlAw78"
      },
      "source": [
        "## Trainign at Scale Using Distributin Strategies API\n",
        "\n",
        "**The Mirrored strategy**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY7FKFCTm1mh",
        "outputId": "ee8009db-889c-485b-d41f-61c70edee881"
      },
      "source": [
        "distribution = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with distribution.scope():\n",
        "  mirrored_model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "  ])\n",
        "  mirrored_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                optimizer=\"sgd\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  \n",
        "batch_size = 100 # must be divisible by the number of replicas\n",
        "history = mirrored_model.fit(X_train, y_train,\n",
        "                             epochs=10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
            "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 1/10\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "1719/1719 [==============================] - 18s 7ms/step - loss: 0.6384 - accuracy: 0.8421\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3374 - accuracy: 0.9064\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2878 - accuracy: 0.9197\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2569 - accuracy: 0.9286\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2339 - accuracy: 0.9348\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2149 - accuracy: 0.9403\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1992 - accuracy: 0.9451\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1855 - accuracy: 0.9487\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.1736 - accuracy: 0.9521\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1631 - accuracy: 0.9549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8KF-QG-CYQC"
      },
      "source": [
        "To load a model and run it on all available devices, we must call `keras.models.load_model()` withing a distribution context like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQnc1OG8CDVT"
      },
      "source": [
        "with distribution.scope():\n",
        "  mirrored_model = keras.models.load_model(\"the_mnist_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIK0UPu0C5Fj"
      },
      "source": [
        "To use a subset of available GPUs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OwzA_xXC9Iz",
        "outputId": "fdfd5e59-8f01-4430-f43a-ba1427627a5a"
      },
      "source": [
        "distribution = tf.distribute.MirroredStrategy([\"/gpu:0\", \"/gpu:1\"])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
            "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2cVfQxBDhXx"
      },
      "source": [
        "**Data parallelism with centralized parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYPw7ug-DLFN",
        "outputId": "a16c8c3a-79da-4814-bb10-b085e6b2485a"
      },
      "source": [
        "distribution = tf.distribute.experimental.CentralStorageStrategy()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3'], variable_device = '/device:CPU:0'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuKqpMh4D1oy"
      },
      "source": [
        "### Training a Model on a TensorFlow Cluster\n",
        "A TensorFlow Cluster is a group of TensorFlow process running in parallel, usually on different machines, and talking to each other to complete some work.\n",
        " \n",
        "<mark>Each TF process in the cluster is called a *task* or a *TF server*.</mark>\n",
        "\n",
        "A TF server or a task has:\n",
        "- IP address,\n",
        "- a port\n",
        "- and a type (or *role* or *job*) which can either be:\n",
        "  - `worker` performs computation, usually on a machine with one or more GPUs.\n",
        "  - `chief` performs computations as well, but it handles extra work such as writing TensorBoard logs or saving checkpoints. There is only one chief in a cluster and if no chief is specified, the first worker is the chief.\n",
        "  - `ps` or *parameter server* which keeps tack of variable values, and it is ususally on a CPU-only machine.\n",
        "  - `evaluator` takes care of evaluation.\n",
        "\n",
        "To create a TensorFlow cluster, we must define cluster specification which is dictionary with one key per job and the values are lists of task addreses. (IP:*port*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXn19xyODtF4"
      },
      "source": [
        "cluster_spec = {\n",
        "    \"worker\": [\n",
        "        \"machine-a-example.com:2222\",    # /job:worker/task:0\n",
        "        \"machine-b-example.com:2222\"     # /job:worker/task:1\n",
        "    ],\n",
        "    \"ps\": [\"machine-a.example.com:2221\"] # /job:ps/task:0\n",
        "}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozn7JjS9P94z"
      },
      "source": [
        "To start a task we need to set the `TF_CONFIG` environment variable for TF Server before starting TensorFlow. It must be JSON encoded dictionary containing:\n",
        "- cluster specification under the `cluster` key.\n",
        "- the type and the index of the current task under the `task` key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cBDBeVjKlVB"
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
        "    \"cluster\": cluster_spec,\n",
        "    \"task\": {\"type\": \"worker\", \"index\": 0}\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjOuBJ8kRBbE"
      },
      "source": [
        "Then run the following training code on every worker:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbFn8IUURGdq"
      },
      "source": [
        "distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
        "\n",
        "with distribution.scope():\n",
        "  mirrored_model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "  ])\n",
        "  mirrored_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "bath_size = 100 # must be divisible by the number of replicas\n",
        "history = mirrored_model.fit(X_train, y_train, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}